{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CGHGHkqE4rJw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3duPlU1fjxPz"
   },
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jovyan\n",
      " * Starting OpenBSD Secure Shell server sshd\n",
      "start-stop-daemon: unable to set gid to 0 (Operation not permitted)\n",
      "   ...fail!\n",
      " * sshd is running\n",
      "Starting namenodes on [localhost]\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: namenode is running as process 163.  Stop it first and ensure /tmp/hadoop-jovyan-namenode.pid file is empty before retry.\n",
      "Starting datanodes\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: datanode is running as process 307.  Stop it first and ensure /tmp/hadoop-jovyan-datanode.pid file is empty before retry.\n",
      "Starting secondary namenodes [615c73bbc755]\n",
      "615c73bbc755: Warning: Permanently added '615c73bbc755' (ED25519) to the list of known hosts.\n",
      "615c73bbc755: secondarynamenode is running as process 497.  Stop it first and ensure /tmp/hadoop-jovyan-secondarynamenode.pid file is empty before retry.\n",
      "Starting resourcemanager\n",
      "resourcemanager is running as process 724.  Stop it first and ensure /tmp/hadoop-jovyan-resourcemanager.pid file is empty before retry.\n",
      "Starting nodemanagers\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: nodemanager is running as process 838.  Stop it first and ensure /tmp/hadoop-jovyan-nodemanager.pid file is empty before retry.\n",
      "WARNING: Use of this script to start the MR JobHistory daemon is deprecated.\n",
      "WARNING: Attempting to execute replacement \"mapred --daemon start\" instead.\n",
      "2656 sun.tools.jps.Jps -lm\n",
      "497 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode\n",
      "163 org.apache.hadoop.hdfs.server.namenode.NameNode\n",
      "307 org.apache.hadoop.hdfs.server.datanode.DataNode\n",
      "724 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\n",
      "838 org.apache.hadoop.yarn.server.nodemanager.NodeManager\n",
      "Configured Capacity: 502392610816 (467.89 GB)\n",
      "Present Capacity: 134942105600 (125.67 GB)\n",
      "DFS Remaining: 134942081024 (125.67 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "DFS Used%: 0.00%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 127.0.0.1:9866 (localhost)\n",
      "Hostname: 615c73bbc755\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 502392610816 (467.89 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "Non DFS Used: 341855133696 (318.38 GB)\n",
      "DFS Remaining: 134942081024 (125.67 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 26.86%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Tue May 09 10:36:53 UTC 2023\n",
      "Last Block Report: Tue May 09 10:30:26 UTC 2023\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n",
      "chmod: changing permissions of '/home/jovyan/jupyter.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/nginx.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/error.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/access.log': Operation not permitted\n"
     ]
    }
   ],
   "source": [
    "! /home/jovyan/start-hadoop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u566smRWkDOS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-05-09 10:37:05,493 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# connect, context, session\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCrxvDnx47f2"
   },
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-tZacCcy49Lv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                Size     Used  Available  Use%\n",
      "hdfs://localhost:9000  467.9 G  291.6 M    125.1 G    0%\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1m8mBXzZ4-kB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwx---   - root   supergroup          0 2023-05-09 10:37 /tmp\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-05-09 10:37 /user\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q1OO-vOyY6L"
   },
   "source": [
    "## Broadcast and accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RjypNMTVyYOp",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[5] at RDD at PythonRDD.scala:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=====================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 2)]\n",
      "errors: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bc = sc.broadcast({\"this\": 0, \"is\": 1, \"text\": 2})\n",
    "errors = sc.accumulator(0)\n",
    "\n",
    "def mapper(x):\n",
    "    global errors\n",
    "    for w in x.split():\n",
    "        if w in bc.value:\n",
    "            yield (bc.value[w], 1)\n",
    "        else:\n",
    "            errors += 1\n",
    "\n",
    "rdd = (\n",
    "    sc\n",
    "   .parallelize([\"this is text\", \"text too\"])\n",
    "   .flatMap(mapper)\n",
    "   .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "print(rdd)\n",
    "print(rdd.collect())\n",
    "print(\"errors:\", errors.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHR7Vzn_jxlF"
   },
   "source": [
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDbdsczvj_i8"
   },
   "source": [
    "RDD is much better and useful than plain MapReduce, but Spark can do even more!\n",
    "Spark DataFrame is table structure over RDDs and can be treated as pandas on steroids.\n",
    "\n",
    "It allows us to perform structured queries and benefit from it. One way is to perform SQL-styled queries (will discuss on next lesson) and another is DataFrame API.\n",
    "\n",
    "Documentation: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EpYvw_HymmTQ",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 2), ('b', 3), ('b', 4)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"a\", 2), (\"b\", 3), (\"b\", 4)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i4mxAmRemmb2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  b|  3|\n",
      "|  b|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = se.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FGJOiQS_2G_-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col_one: string (nullable = true)\n",
      " |-- col_two: long (nullable = true)\n",
      "\n",
      "+-------+-------+\n",
      "|col_one|col_two|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      a|      2|\n",
      "|      b|      3|\n",
      "|      b|      4|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = se.createDataFrame(\n",
    "    rdd.map(lambda x: Row(col_one=x[0], col_two=x[1]))\n",
    ")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "u1RymT0N7n8z",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|col_one|\n",
      "+-------+\n",
      "|      a|\n",
      "|      a|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['col_one']).limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wwkvZly3774s",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(['col_one']).distinct().rdd.map(lambda x: x.col_one).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs: https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "D_QIAytR70kc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:====================================>                   (21 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|col_one|col_two|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      a|      2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "(\n",
    "  df.select(['col_one', 'col_two'])\n",
    "    .where(F.col('col_one') == 'a')\n",
    "    .limit(2)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rNp94hJnjHqW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|col_one|col_two|col_two_float|\n",
      "+-------+-------+-------------+\n",
      "|      a|      1|          1.0|\n",
      "|      a|      2|          2.0|\n",
      "|      b|      3|          3.0|\n",
      "|      b|      4|          4.0|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select('*', df['col_two'].cast('float').alias('col_two_float'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fMgcNpPmjTUM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:======================================>                 (22 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|col_one|col_two_square|\n",
      "+-------+--------------+\n",
      "|      b|          16.0|\n",
      "|      b|           9.0|\n",
      "|      a|           4.0|\n",
      "|      a|           1.0|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "square_df = df.select('col_one', (df['col_two_float'] * df['col_two_float']).alias('col_two_square'))\n",
    "square_df.orderBy('col_two_square', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6TO3OLMN7W51",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=============================================>          (26 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|col_one|col_two_list|\n",
      "+-------+------------+\n",
      "|      a|      [2, 1]|\n",
      "|      b|      [3, 4]|\n",
      "+-------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "(\n",
    "    df\n",
    "      .groupby('col_one')\n",
    "      .agg(F.collect_list(\"col_two\").alias(\"col_two_list\"))\n",
    "      .select(['col_one', 'col_two_list'])\n",
    "      .limit(10)\n",
    "      .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "lc31NQ0x2Lv-"
   },
   "source": [
    "# convertable to Pandas\n",
    "pandas_df = df.toPandas()\n",
    "pandas_df\n",
    "\n",
    "# load from Pandas\n",
    "df = se.createDataFrame(pandas_df)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_one='a', col_two=1, col_two_float=1.0),\n",
       " Row(col_one='a', col_two=2, col_two_float=2.0),\n",
       " Row(col_one='b', col_two=3, col_two_float=3.0),\n",
       " Row(col_one='b', col_two=4, col_two_float=4.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.toLocalIterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = se.createDataFrame(df.toLocalIterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col_one: string (nullable = true)\n",
      " |-- col_two: long (nullable = true)\n",
      " |-- col_two_float: double (nullable = true)\n",
      "\n",
      "+-------+-------+-------------+\n",
      "|col_one|col_two|col_two_float|\n",
      "+-------+-------+-------------+\n",
      "|      a|      1|          1.0|\n",
      "|      a|      2|          2.0|\n",
      "|      b|      3|          3.0|\n",
      "|      b|      4|          4.0|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PYlbgkZI3QiH",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(col_one='a', col_two=1, col_two_float=1.0),\n",
       " Row(col_one='a', col_two=2, col_two_float=2.0),\n",
       " Row(col_one='b', col_two=3, col_two_float=3.0),\n",
       " Row(col_one='b', col_two=4, col_two_float=4.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also can get RDD from DF\n",
    "df.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi3keNw6mteI"
   },
   "source": [
    "## Data formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Mt8434r9mvr2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We may want to operate with not just plain text, but something more complex\n",
    "# For example, Parquet - it can be useful for huge datasets for faster calcs\n",
    "df.write.save(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zxDJLG6pmvwf",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_one='a', col_two=1, col_two_float=1.0),\n",
       " Row(col_one='a', col_two=2, col_two_float=2.0),\n",
       " Row(col_one='b', col_two=3, col_two_float=3.0),\n",
       " Row(col_one='b', col_two=4, col_two_float=4.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = se.read.parquet(\"data.parquet\")\n",
    "data.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1NJypwklF_Z"
   },
   "source": [
    "## Outbrain click prediction dataseet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twX4Yeez5Loj"
   },
   "source": [
    "https://www.kaggle.com/c/outbrain-click-prediction/data - you need to register here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.5.12-py3-none-any.whl\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.12\n",
      "    Uninstalling kaggle-1.5.12:\n",
      "      Successfully uninstalled kaggle-1.5.12\n",
      "Successfully installed kaggle-1.5.12\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/Kaggle/kaggle-api\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# instruction where to get ~/.kaggle/kaggle.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/jovyan/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/.kaggle/kaggle.json\n",
    "{\"username\":\"ashishparmar\",\"key\":\"###\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "j4FsSCqe5CvG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (1.24.3)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: kaggle==1.5.3 in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (1.16.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (2.28.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (4.65.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle==1.5.3) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle==1.5.3) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle==1.5.3) (3.4)\n",
      "403 - Forbidden\n",
      "403 - Forbidden\n",
      "403 - Forbidden\n",
      "403 - Forbidden\n",
      "403 - Forbidden\n",
      "403 - Forbidden\n",
      "403 - Forbidden\n",
      "403 - Forbidden\n",
      "403 - Forbidden\n"
     ]
    }
   ],
   "source": [
    "! pip install -U urllib3 kaggle==1.5.3\n",
    "! kaggle competitions download -c outbrain-click-prediction -f page_views_sample.csv.zip\n",
    "! kaggle competitions download -c outbrain-click-prediction -f events.csv.zip \n",
    "! kaggle competitions download -c outbrain-click-prediction -f documents_topics.csv.zip \n",
    "! kaggle competitions download -c outbrain-click-prediction -f documents_entities.csv.zip \n",
    "! kaggle competitions download -c outbrain-click-prediction -f documents_meta.csv.zip \n",
    "! kaggle competitions download -c outbrain-click-prediction -f clicks_test.csv.zip\n",
    "! kaggle competitions download -c outbrain-click-prediction -f clicks_train.csv.zip\n",
    "! kaggle competitions download -c outbrain-click-prediction -f documents_categories.csv.zip\n",
    "! kaggle competitions download -c outbrain-click-prediction -f promoted_content.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  documents_topics.csv.zip\n",
      "  inflating: documents_topics.csv    \n",
      "\n",
      "Archive:  documents_categories.csv.zip\n",
      "  inflating: documents_categories.csv  \n",
      "\n",
      "Archive:  events.csv.zip\n",
      "  inflating: events.csv              \n",
      "\n",
      "Archive:  clicks_train.csv.zip\n",
      "  inflating: clicks_train.csv        \n",
      "\n",
      "Archive:  documents_entities.csv.zip\n",
      "  inflating: documents_entities.csv  \n",
      "\n",
      "Archive:  documents_meta.csv.zip\n",
      "  inflating: documents_meta.csv      \n",
      "\n",
      "Archive:  page_views_sample.csv.zip\n",
      "  inflating: page_views_sample.csv   \n",
      "\n",
      "Archive:  clicks_test.csv.zip\n",
      "  inflating: clicks_test.csv         \n",
      "\n",
      "8 archives were successfully processed.\n"
     ]
    }
   ],
   "source": [
    "! unzip '*.zip'\n",
    "! rm -rf *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `page_views_sample.csv': File exists\n",
      "put: `documents_topics.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put page_views_sample.csv\n",
    "! hdfs dfs -put documents_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "V5Qx5EkolI_0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "|8205775c5387f9|        120| 44196592|       1|       IN>16|             2|\n",
      "|9cb0ccd8458371|        120| 65817371|       1|   US>CA>807|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "pv = se.read.csv(\"page_views_sample.csv\", header=True)\n",
    "pv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------------+\n",
      "|document_id|topic_id|  confidence_level|\n",
      "+-----------+--------+------------------+\n",
      "|    1595802|     140|0.0731131601068925|\n",
      "|    1595802|      16|0.0594164867373976|\n",
      "|    1595802|     143|0.0454207537554526|\n",
      "|    1595802|     170|0.0388674285182961|\n",
      "|    1524246|     113| 0.196450402209685|\n",
      "+-----------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = se.read.csv(\"documents_topics.csv\", header=True)\n",
    "dt.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hK93Ci6Rj51c"
   },
   "source": [
    "## Evaluation Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Jf8VBI-j530"
   },
   "source": [
    "Data: outbrain click prediction\n",
    "\n",
    "Tasks:\n",
    "Using Spark RDD, DataFrame API and Python, calculate:\n",
    "\n",
    "**1**. Top 10 most visited document_ids in the page_views_sample log\n",
    "\n",
    "**2**. How many users have at least 2 different traffic_sources in the page_views_sample log (note the value is not a count, it's an encoded enum)\n",
    "\n",
    "**3***. Top 10 most visited topic_ids in page_views_sample log (use documents_topics table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Top 10 most visited document ids in the page_views_sample log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1811567,\n",
       " 234,\n",
       " 42744,\n",
       " 1858440,\n",
       " 1780813,\n",
       " 60164,\n",
       " 1790442,\n",
       " 1877626,\n",
       " 1821895,\n",
       " 732651]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10 = (\n",
    "    pv.groupBy(\"document_id\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .limit(10)\n",
    "    .rdd.map(lambda row: int(row.document_id))\n",
    "    .collect()\n",
    ")\n",
    "top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Number of users who have atleast 2 different traffic sources in the page_views_sample log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98080"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "users = (\n",
    "    pv.groupBy(\"uuid\")\n",
    "    .agg(countDistinct(\"traffic_source\").alias(\"distinct_traffic_sources\"))\n",
    "    .filter(\"distinct_traffic_sources >= 2\")\n",
    "    .count()\n",
    ")\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top 10 most visited topic ids in page_views_sample log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20, 16, 216, 136, 140, 143, 36, 97, 8, 269]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df = pv.join(dt, on=\"document_id\", how=\"inner\")\n",
    "\n",
    "top_10_ = (\n",
    "    joined_df.groupBy(\"topic_id\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .limit(10)\n",
    "    .rdd.map(lambda row: int(row.topic_id))\n",
    "    .collect()\n",
    ")\n",
    "top_10_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission format is the result.json json file with top_10_documents, users and top_10_topics keys.\n",
    "For TOP-10 results, the answer must be written in the form of a sheet ordered from TOP-1 to TOP-10 with an id.\n",
    "\n",
    "result.json example:\n",
    "\n",
    "    {\n",
    "        \"top_10_documents\": [\n",
    "            111,\n",
    "            222,\n",
    "            333,\n",
    "            ...,\n",
    "            1010\n",
    "        ],\n",
    "        \"users\": 10000,\n",
    "        \"top_10_topics\": [\n",
    "            11,\n",
    "            22,\n",
    "            33,\n",
    "            ...,\n",
    "            101\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result = {\n",
    "    \"top_10_documents\": top_10,\n",
    "    \"users\": users,\n",
    "    \"top_10_topics\": top_10_,\n",
    "}\n",
    "\n",
    "with open(\"result.json\", \"w\") as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"top_10_documents\": [1811567, 234, 42744, 1858440, 1780813, 60164, 1790442, 1877626, 1821895, 732651], \"users\": 98080, \"top_10_topics\": [20, 16, 216, 136, 140, 143, 36, 97, 8, 269]}"
     ]
    }
   ],
   "source": [
    "! cat result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "! curl -F file=@result.json \"51.250.54.133:80/MDS-LSML1/unknown_14/w4/1\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_seminar (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
